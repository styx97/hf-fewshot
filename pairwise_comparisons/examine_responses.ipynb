{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('/fs/clip-political/rupak/hf-fewshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hf_fewshot.prompting_utils import load_jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    'A' : '1', \n",
    "    'B' : '2'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a class of models to analyze their outputs - 8B or 70B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/draw-0-results_70B.jsonl',\n",
       " 'output/draw-1-results_70B.jsonl',\n",
       " 'output/draw-2-results_70B.jsonl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_files = sorted(glob(\"output/*70B.jsonl\"))\n",
    "result_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_jsonlines(result_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = [x['preferences']['1'] for x in data]\n",
    "mean_2 = [x['preferences']['2'] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5663463155691262, 0.4336536815066035)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean_1), np.mean(mean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06634631556912618, -0.0663463184933965)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean_1) - 0.50, np.mean(mean_2) - 0.50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1365568787.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[34], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    llama 8B biases - 0.14, 0.024, 0.1\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "llama 8B biases - 0.14, 0.024, 0.1 \n",
    "llama 70B biases - 0.06, 0.14, 0.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the outputs of the models into a single dataframe, track their source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for file in result_files:\n",
    "    results = load_jsonlines(file)\n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "    results['source'] = Path(file).name.split(\"-\")[1]\n",
    "\n",
    "    results[\"label\"] = results[\"label\"].apply(lambda x: label_dict[x])\n",
    "    results[\"correct\"] = results[\"label\"] == results[\"output\"]\n",
    "    df_list.append({\n",
    "        # keep just the name of the file\n",
    "        \"file\": Path(file).name,\n",
    "        \"results_df\": results\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_score(df, endswith=\"_0\"): \n",
    "    # take a subset of the df where the pair_id ends with _0 \n",
    "    df_0 = df[df[\"pair_id\"].str.endswith(endswith)]\n",
    "    print(df_0[\"correct\"].mean()) \n",
    "\n",
    "bias = 0.06\n",
    "def debias_dict(preferences, bias):\n",
    "    # debias the preferences by subtracting the bias from the value \n",
    "    preferences['1'] = preferences['1'] - bias\n",
    "    preferences['2'] = preferences['2'] + bias\n",
    "    return preferences\n",
    "\n",
    "\n",
    "def get_debiased_df(df_temp, bias):\n",
    "    # apply debias_dict to the preferences column for each row in df \n",
    "    df= df_temp.copy(deep=True)\n",
    "\n",
    "    df[\"preferences_debiased\"] = df[\"preferences\"].apply(lambda x: debias_dict(x, bias))\n",
    "    df['output_debiased'] = df[\"preferences_debiased\"].apply(lambda x: '1' if x['1'] > x['2'] else '2')\n",
    "    df['correct_debiased'] = df['label'] == df['output_debiased']\n",
    "\n",
    "    return df \n",
    "    \n",
    "def get_default_score_debiased(df, endswith=\"_0\"): \n",
    "    # take a subset of the df where the pair_id ends with _0 \n",
    "    df_temp = df[df[\"pair_id\"].str.endswith(endswith)]\n",
    "    print(df_temp[\"correct_debiased\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df_list[0][\"results_df\"]\n",
    "df1 = df_list[1][\"results_df\"]\n",
    "df2 = df_list[2][\"results_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_debiased = get_debiased_df(df0, 0.06)\n",
    "df1_debiased = get_debiased_df(df1, 0.14)\n",
    "df2_debiased = get_debiased_df(df2, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6319702602230484\n",
      "0.5966542750929368\n",
      "0.6282527881040892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_default_score(df0), get_default_score(df1), get_default_score(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the dfs\n",
    "df_combined = pd.concat([df0, df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>output</th>\n",
       "      <th>preferences</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1047895789_0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'1': -0.05999847058757041, '2': 1.05999839067...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1047895789_1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'1': 0.9399939203262329, '2': 0.0600060489205...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1047895789_2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'1': 0.9399995231628417, '2': 0.0600004888304...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1047895789_3</td>\n",
       "      <td>2</td>\n",
       "      <td>{'1': -0.056175516163930295, '2': 1.0561755871...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1047887035_0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'1': -0.04201378807425499, '2': 1.04201370239...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pair_id output                                        preferences  \\\n",
       "0  1047895789_0      2  {'1': -0.05999847058757041, '2': 1.05999839067...   \n",
       "1  1047895789_1      1  {'1': 0.9399939203262329, '2': 0.0600060489205...   \n",
       "2  1047895789_2      1  {'1': 0.9399995231628417, '2': 0.0600004888304...   \n",
       "3  1047895789_3      2  {'1': -0.056175516163930295, '2': 1.0561755871...   \n",
       "4  1047887035_0      2  {'1': -0.04201378807425499, '2': 1.04201370239...   \n",
       "\n",
       "  label source  correct  \n",
       "0     1      0    False  \n",
       "1     2      0    False  \n",
       "2     2      0    False  \n",
       "3     1      0    False  \n",
       "4     1      0    False  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>output</th>\n",
       "      <th>preferences</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>correct</th>\n",
       "      <th>preferences_debiased</th>\n",
       "      <th>output_debiased</th>\n",
       "      <th>correct_debiased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1047895789_0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'1': -0.05999847058757041, '2': 1.05999839067...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'1': -0.05999847058757041, '2': 1.05999839067...</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1047895789_1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'1': 0.9399939203262329, '2': 0.0600060489205...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'1': 0.9399939203262329, '2': 0.0600060489205...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1047895789_2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'1': 0.9399995231628417, '2': 0.0600004888304...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'1': 0.9399995231628417, '2': 0.0600004888304...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1047895789_3</td>\n",
       "      <td>2</td>\n",
       "      <td>{'1': -0.056175516163930295, '2': 1.0561755871...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'1': -0.056175516163930295, '2': 1.0561755871...</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1047887035_0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'1': -0.04201378807425499, '2': 1.04201370239...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'1': -0.04201378807425499, '2': 1.04201370239...</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pair_id output                                        preferences  \\\n",
       "0  1047895789_0      2  {'1': -0.05999847058757041, '2': 1.05999839067...   \n",
       "1  1047895789_1      1  {'1': 0.9399939203262329, '2': 0.0600060489205...   \n",
       "2  1047895789_2      1  {'1': 0.9399995231628417, '2': 0.0600004888304...   \n",
       "3  1047895789_3      2  {'1': -0.056175516163930295, '2': 1.0561755871...   \n",
       "4  1047887035_0      2  {'1': -0.04201378807425499, '2': 1.04201370239...   \n",
       "\n",
       "  label source  correct                               preferences_debiased  \\\n",
       "0     1      0    False  {'1': -0.05999847058757041, '2': 1.05999839067...   \n",
       "1     2      0    False  {'1': 0.9399939203262329, '2': 0.0600060489205...   \n",
       "2     2      0    False  {'1': 0.9399995231628417, '2': 0.0600004888304...   \n",
       "3     1      0    False  {'1': -0.056175516163930295, '2': 1.0561755871...   \n",
       "4     1      0    False  {'1': -0.04201378807425499, '2': 1.04201370239...   \n",
       "\n",
       "  output_debiased  correct_debiased  \n",
       "0               2             False  \n",
       "1               1             False  \n",
       "2               1             False  \n",
       "3               2             False  \n",
       "4               2             False  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0_debiased.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joint_debiased(df_combined): \n",
    "    unique_pairs = list(set([elem.split(\"_\")[0] for elem in df_combined[\"pair_id\"].values])) \n",
    "\n",
    "    labels = []\n",
    "    gold_labels = []\n",
    "    for pair in unique_pairs: \n",
    "        df_temp = df_combined[(df_combined[\"pair_id\"].str.startswith(pair)) & (df_combined[\"pair_id\"].str.endswith(\"_0\"))]\n",
    "        # find the majority value of the correct_debiased column\n",
    "        majority = df_temp[\"output_debiased\"].value_counts().idxmax()\n",
    "        gold_label = df_temp[\"label\"].values[0]\n",
    "\n",
    "        labels.append(majority)\n",
    "        gold_labels.append(gold_label)\n",
    "\n",
    "    return labels, gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_debiased = pd.concat([df0_debiased, df1_debiased, df2_debiased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "debiased_labels, debiased_gold_labels =  get_joint_debiased(df_combined_debiased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.39      0.47       238\n",
      "           2       0.62      0.80      0.70       300\n",
      "\n",
      "    accuracy                           0.62       538\n",
      "   macro avg       0.61      0.59      0.59       538\n",
      "weighted avg       0.61      0.62      0.60       538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(debiased_gold_labels, debiased_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_sum(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe, add the preferences of all the columns and return the dict \n",
    "    \"\"\"\n",
    "    preferences = df[\"preferences\"].values\n",
    "    preference_sum = {'1': 0, '2': 0}\n",
    "    for pref in preferences: \n",
    "        preference_sum['1'] += pref['1']\n",
    "        preference_sum['2'] += pref['2']\n",
    "    return preference_sum\n",
    "\n",
    "\n",
    "def get_augmentation_bias(df_combined): \n",
    "    \"\"\"\n",
    "    Adds up the scores of the 1st and 4th row, and the second and 3rd row for the 12 preferences of each unique pair \n",
    "    @Hauke: This is the debiasing method that we discussed in the meeting earlier. \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    unique_pairs = list(set([elem.split(\"_\")[0] for elem in df_combined[\"pair_id\"].values])) \n",
    "         \n",
    "    labels = []\n",
    "    gold_labels = []\n",
    "    # for each unique pair, get the preferences for each of the 3 dataframes\n",
    "    for elem in unique_pairs: \n",
    "        # for an unique pair, there are 12 preferences\n",
    "        all_preferences = []\n",
    "        \n",
    "        df_temp = df_combined[df_combined[\"pair_id\"].str.startswith(elem)].sort_values(by=\"pair_id\")\n",
    "        # label of the first row is the label \n",
    "        gold_label = df_temp.iloc[0][\"label\"]\n",
    "\n",
    "        # add up preferences of all rows with pair_id ending with _0 or _3\n",
    "        df_0_3 = df_temp[df_temp[\"pair_id\"].str.endswith(\"_0\") | df_temp[\"pair_id\"].str.endswith(\"_3\")]\n",
    "        df_1_2 = df_temp[df_temp[\"pair_id\"].str.endswith(\"_1\") | df_temp[\"pair_id\"].str.endswith(\"_2\")]        \n",
    "\n",
    "        preference_sum_0_3 = preference_sum(df_0_3)\n",
    "        preference_sum_1_2 = preference_sum(df_1_2)\n",
    "\n",
    "        score_1 = preference_sum_0_3['1'] + preference_sum_1_2['2']\n",
    "        score_2 = preference_sum_0_3['2'] + preference_sum_1_2['1']\n",
    "\n",
    "        if score_1 > score_2: \n",
    "            labels.append('1')\n",
    "        else:\n",
    "            labels.append('2')\n",
    "\n",
    "        gold_labels.append(gold_label)\n",
    "\n",
    "    return labels, gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6319702602230484\n",
      "0.5966542750929368\n",
      "0.6282527881040892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the default win rate\n",
    "get_default_score(df0), get_default_score(df1), get_default_score(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, gold_labels = get_augmentation_bias(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.50      0.54       238\n",
      "           2       0.65      0.74      0.69       300\n",
      "\n",
      "    accuracy                           0.63       538\n",
      "   macro avg       0.62      0.62      0.62       538\n",
      "weighted avg       0.63      0.63      0.62       538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate precision, recall, f1, accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(gold_labels, labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pairwise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
